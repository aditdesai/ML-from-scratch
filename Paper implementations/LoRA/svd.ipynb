{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1303437f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a rank-deficient (not full rank: has linearly dependent rows/columns) matrix W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0797,  0.5545,  0.8058, -0.7140, -0.1518,  1.0773,  2.3690,  0.8486,\n",
       "         -1.1825, -3.2632],\n",
       "        [-0.3303,  0.2283,  0.4145, -0.1924, -0.0215,  0.3276,  0.7926,  0.2233,\n",
       "         -0.3422, -0.9614],\n",
       "        [-0.5256,  0.9864,  2.4447, -0.0290,  0.2305,  0.5000,  1.9831, -0.0311,\n",
       "         -0.3369, -1.1376],\n",
       "        [ 0.7900, -1.1336, -2.6746,  0.1988, -0.1982, -0.7634, -2.5763, -0.1696,\n",
       "          0.6227,  1.9294],\n",
       "        [ 0.1258,  0.1458,  0.5090,  0.1768,  0.1071, -0.1327, -0.0323, -0.2294,\n",
       "          0.2079,  0.5128],\n",
       "        [ 0.7697,  0.0050,  0.5725,  0.6870,  0.2783, -0.7818, -1.2253, -0.8533,\n",
       "          0.9765,  2.5786],\n",
       "        [ 1.4157, -0.7814, -1.2121,  0.9120,  0.1760, -1.4108, -3.1692, -1.0791,\n",
       "          1.5325,  4.2447],\n",
       "        [-0.0119,  0.6050,  1.7245,  0.2584,  0.2528, -0.0086,  0.7198, -0.3620,\n",
       "          0.1865,  0.3410],\n",
       "        [ 1.0485, -0.6394, -1.0715,  0.6485,  0.1046, -1.0427, -2.4174, -0.7615,\n",
       "          1.1147,  3.1054],\n",
       "        [ 0.9088,  0.1936,  1.2136,  0.8946,  0.4084, -0.9295, -1.2294, -1.1239,\n",
       "          1.2155,  3.1628]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, k = 10, 10\n",
    "\n",
    "W_rank = 2\n",
    "W = torch.randn(d, W_rank) @ torch.randn(W_rank, k)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the rank of matrix W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of W: 2\n"
     ]
    }
   ],
   "source": [
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(f'Rank of W: {W_rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is SVD?\n",
    "At its core, SVD is a way to decompose any matrix into three simpler matrices that, when multiplied together, reproduce the original matrix.\n",
    "For any matrix W of dimensions m×n, SVD expresses it as:\n",
    "W = U × Σ × V^T\n",
    "Where:\n",
    "\n",
    "U is an m×m orthogonal matrix (its columns are orthonormal eigenvectors of WW^T)\n",
    "Σ (Sigma) is an m×n diagonal matrix with non-negative real numbers on the diagonal (the singular values)\n",
    "V^T is the transpose of an n×n orthogonal matrix (columns of V are orthonormal eigenvectors of W^TW)\n",
    "\n",
    "### The Geometric Interpretation\n",
    "SVD provides a powerful geometric interpretation of matrix transformations:\n",
    "\n",
    "V^T represents a rotation in the input space\n",
    "Σ represents a scaling along the coordinate axes\n",
    "U represents a rotation in the output space\n",
    "\n",
    "Think of it as: the matrix W takes vectors from one space, rotates them (V^T), stretches or compresses them along certain directions (Σ), and then rotates them again (U).\n",
    "\n",
    "### Why Singular Values Matter\n",
    "The singular values (the diagonal elements of Σ) are sorted in descending order and represent the \"importance\" of each dimension. They tell us how much the matrix stretches space along each principal direction.\n",
    "The number of non-zero singular values equals the rank of the matrix. This is crucial for low-rank approximations.\n",
    "\n",
    "### Low-Rank Approximations\n",
    "One of the most powerful applications of SVD is creating low-rank approximations of matrices. If we keep only the r largest singular values (and corresponding columns of U and V), we get the best possible rank-r approximation to the original matrix in terms of minimizing the Frobenius norm of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of B: torch.Size([10, 2])\n",
      "Shape of A: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Perform SVD on W (W = U x S x V^T)\n",
    "U, S, V = torch.svd(W) # U = (10, 10), S = (10,), V = (10, 10)\n",
    "\n",
    "# For rank-r factorization, keep only the first r singular values (and corresponding columns of U and V)\n",
    "U_r = U[:, :W_rank]\n",
    "S_r = torch.diag(S[:W_rank])\n",
    "V_r = V[:, :W_rank].t()\n",
    "\n",
    "# Compute A and B\n",
    "B = U_r @ S_r\n",
    "A = V_r\n",
    "\n",
    "print(f'Shape of B: {B.shape}')\n",
    "print(f'Shape of A: {A.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same input, check the output using the original W matrix and the matrices resulting from the decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y using W:\n",
      "tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1639e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n",
      "Original y using BA:\n",
      "tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1640e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n"
     ]
    }
   ],
   "source": [
    "bias = torch.randn(d)\n",
    "x = torch.randn(d)\n",
    "\n",
    "# Compute y = Wx + b\n",
    "y = W @ x + bias\n",
    "\n",
    "# Compute y' = BAx + b\n",
    "y_prime = (B @ A) @ x + bias\n",
    "\n",
    "print(f'Original y using W:\\n{y}')\n",
    "print(f'Original y using BA:\\n{y_prime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output implies that A and B captured most of the information of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters of W: 100\n",
      "Total parameters of A and B: 40\n"
     ]
    }
   ],
   "source": [
    "print(f'Total parameters of W: {W.nelement()}')\n",
    "print(f'Total parameters of A and B: {A.nelement() + B.nelement()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
